# Importing the necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn import linear_model
from sklearn import tree
from sklearn import ensemble
from sklearn import svm
import xgboost
Importing the Dataset

data = pd.read_csv('traffic volume.csv')
#Analyse The Data

data.head()
holiday	temp	rain	snow	weather	date	Time	traffic_volume
0	NaN	288.28	0.0	0.0	Clouds	02-10-2012	09:00:00	5545
1	NaN	289.36	0.0	0.0	Clouds	02-10-2012	10:00:00	4516
2	NaN	289.58	0.0	0.0	Clouds	02-10-2012	11:00:00	4767
3	NaN	290.13	0.0	0.0	Clouds	02-10-2012	12:00:00	5026
4	NaN	291.14	0.0	0.0	Clouds	02-10-2012	13:00:00	4918
data.describe()
temp	rain	snow	traffic_volume
count	48151.000000	48202.000000	48192.000000	48204.000000
mean	281.205351	0.334278	0.000222	3259.818355
std	13.343675	44.790062	0.008169	1986.860670
min	0.000000	0.000000	0.000000	0.000000
25%	272.160000	0.000000	0.000000	1193.000000
50%	282.460000	0.000000	0.000000	3380.000000
75%	291.810000	0.000000	0.000000	4933.000000
max	310.070000	9831.300000	0.510000	7280.000000
data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 48204 entries, 0 to 48203
Data columns (total 8 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   holiday         61 non-null     object 
 1   temp            48151 non-null  float64
 2   rain            48202 non-null  float64
 3   snow            48192 non-null  float64
 4   weather         48155 non-null  object 
 5   date            48204 non-null  object 
 6   Time            48204 non-null  object 
 7   traffic_volume  48204 non-null  int64  
dtypes: float64(3), int64(1), object(4)
memory usage: 2.9+ MB
Handling Missing Values

data.isnull().sum()
holiday           48143
temp                 53
rain                  2
snow                 12
weather              49
date                  0
Time                  0
traffic_volume        0
dtype: int64
data['temp'].fillna(data['temp'].mean())
data['rain'].fillna(data['rain'].mean())
data['snow'].fillna(data['snow'].mean())
0        0.0
1        0.0
2        0.0
3        0.0
4        0.0
        ... 
48199    0.0
48200    0.0
48201    0.0
48202    0.0
48203    0.0
Name: snow, Length: 48204, dtype: float64
from collections import Counter
print(Counter(data['weather']))
Counter({'Clouds': 15144, 'Clear': 13383, 'Mist': 5942, 'Rain': 5665, 'Snow': 2875, 'Drizzle': 1818, 'Haze': 1359, 'Thunderstorm': 1033, 'Fog': 912, nan: 49, 'Smoke': 20, 'Squall': 4})
Data Visualization

data.corr
data = data.select_dtypes(include=["number"])  # Keep only numeric columns
corr = data.corr()  # Correct
sns.heatmap(corr)
<Axes: >

import pandas as pd
data = pd.read_csv('traffic volume.csv')
sns.pairplot(data)
<seaborn.axisgrid.PairGrid at 0x1efa8067e00>

data.boxplot()
<Axes: >

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['weather'] = le.fit_transform(data['weather'])
data.head()
holiday	temp	rain	snow	weather	date	Time	traffic_volume
0	NaN	288.28	0.0	0.0	1	02-10-2012	09:00:00	5545
1	NaN	289.36	0.0	0.0	1	02-10-2012	10:00:00	4516
2	NaN	289.58	0.0	0.0	1	02-10-2012	11:00:00	4767
3	NaN	290.13	0.0	0.0	1	02-10-2012	12:00:00	5026
4	NaN	291.14	0.0	0.0	1	02-10-2012	13:00:00	4918
data['temp'] = le.fit_transform(data['temp'])
data[["day", "month", "year"]] = data["date"].str.split("-", expand=True)
data[["hours", "minutes", "seconds"]] = data["Time"].str.split(":", expand=True)
data.drop(columns=['date', 'Time'], axis=1, inplace=True)
data.head()
holiday	temp	rain	snow	weather	traffic_volume	day	month	year	hours	minutes	seconds
0	NaN	4025	0.0	0.0	1	5545	02	10	2012	09	00	00
1	NaN	4145	0.0	0.0	1	4516	02	10	2012	10	00	00
2	NaN	4168	0.0	0.0	1	4767	02	10	2012	11	00	00
3	NaN	4229	0.0	0.0	1	5026	02	10	2012	12	00	00
4	NaN	4346	0.0	0.0	1	4918	02	10	2012	13	00	00
Splitting the Dataset into Dependent and Independent variable

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
y = data.drop(columns=['traffic_volume'], axis=1)
x = data.drop(columns=['traffic_volume'], axis=1)
Feature Scaling

names = x.columns
x = pd.DataFrame(x, columns = names)
x = pd.DataFrame(y, columns= names)  # Ensure correct column name
x.head()
holiday	temp	rain	snow	weather	day	month	year	hours	minutes	seconds
0	NaN	4025	0.0	0.0	1	02	10	2012	09	00	00
1	NaN	4145	0.0	0.0	1	02	10	2012	10	00	00
2	NaN	4168	0.0	0.0	1	02	10	2012	11	00	00
3	NaN	4229	0.0	0.0	1	02	10	2012	12	00	00
4	NaN	4346	0.0	0.0	1	02	10	2012	13	00	00
# Define Features and Target Variable
x = data.drop(columns=['traffic_volume'])  # Features
y = data['traffic_volume']  # Target
x.shape
(48204, 11)
y.shape
(48204,)
print(x.dtypes)
holiday     object
temp         int64
rain       float64
snow       float64
weather      int64
day         object
month       object
year        object
hours       object
minutes     object
seconds     object
dtype: object
Splitting the data into Train and Test

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
categorical_columns = ['holiday', 'temp', 'rain', 'snow', 'weather', 'year', 'month', 'day', 'hours', 'minutes', 'seconds']
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    x[col] = le.fit_transform(x[col])  # Convert categorical to numeric
    label_encoders[col] = le  # Store encoders for later use
# Splitting dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
x_train.shape
(38563, 11)
Model Building

Training and Testing the Model

# Model Initializations
lin_reg = linear_model.LinearRegression()
Dtree = tree.DecisionTreeRegressor()
Rand = ensemble.RandomForestRegressor(n_estimators=100, random_state=42)
svr = svm.SVR()
XGB = xgboost.XGBRegressor()
print(x_train.isnull().sum())  # Check for missing values in each column
print(y_train.isnull().sum())  # Check for missing values in target variable
holiday    0
temp       0
rain       0
snow       0
weather    0
day        0
month      0
year       0
hours      0
minutes    0
seconds    0
dtype: int64
0
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")  # Options: "median", "most_frequent"
x_train = imputer.fit_transform(x_train)# Fills missing values with mean
x_test = imputer.transform(x_test)
# Train models
lin_reg.fit(x_train, y_train)
Dtree.fit(x_train, y_train)
Rand.fit(x_train, y_train)
svr.fit(x_train, y_train)
XGB.fit(x_train, y_train)
XGBRegressor(base_score=None, booster=None, callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=None,
             num_parallel_tree=None, random_state=None, ...)
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
p1 = lin_reg.predict(x_train)
p2 = Dtree.predict(x_train)
p3 = Rand.predict(x_train)
p4 = svr.predict(x_train)
p5 = XGB.predict(x_train)
Model Evaluation

from sklearn import metrics
print(metrics.r2_score(p1, y_train))
print(metrics.r2_score(p2, y_train))
print(metrics.r2_score(p3, y_train))
print(metrics.r2_score(p4, y_train))
print(metrics.r2_score(p5, y_train))
-5.45898314059456
1.0
0.9747230692401472
-58.11845129400455
0.8460580706596375
x_train = np.nan_to_num(x_train, nan=np.nanmean(x_train))
x_test = np.nan_to_num(x_test, nan=np.nanmean(x_test))
p1 = lin_reg.predict(x_test)
p2 = Dtree.predict(x_test)
p3 = Rand.predict(x_test)
p4 = svr.predict(x_test)
p5 = XGB.predict(x_test)
print(metrics.r2_score(p1, y_test))
print(metrics.r2_score(p2, y_test))
print(metrics.r2_score(p3, y_test))
print(metrics.r2_score(p4, y_test))
print(metrics.r2_score(p5, y_test))
-5.326830630340053
0.6833687058990447
0.8019717048784262
-56.8140817039808
0.8068752288818359
RMSE â€“Root Mean Square Error

from sklearn.metrics import mean_squared_error
MSE = mean_squared_error(p3, y_test)
np.sqrt(MSE)
np.float64(800.7602451294027)
import pickle
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(x_train, y_train)
RandomForestRegressor(random_state=42)
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x_test)  # Fit on the current dataset
pickle.dump(model, open("model.pkl", "wb"))
pickle.dump(scaler, open("encoder.pkl", "wb"))
